---
title: "Simple Linear Regression"
teaching: 45
exercises: 10
questions:
- "What is linear regression?"
- "What are the assumptions we need to meet to use linear regression?"
- "How do I perform linear regression in R?"
- "How do I ensure that my data meets the model assumptions?"
objectives:
- "Understand theory of linear regression."
- "Use linear regression on our dataset."
- "Check the model diagnostics to ensure the data meets the assumptions of 
linear regression."
keypoints:
- 
output: html_document
---

<script
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript">
</script>


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)

# Function to limit number of lines printed
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

setwd("~/git/LinearRegressionR/_episodes_rmd")
library(ggplot2)
library(GGally)
data <- read.csv("data/data.csv",sep=",", header=TRUE, stringsAsFactors = TRUE)
data$ID <-as.factor(data$ID)
data$sex <- factor(data$sex,levels=c(0,1), labels = c( "Female","Male"))
data$fbs <- factor(data$fbs,levels=c(0,1), labels = c("<120", ">120"))
data$restecg <- factor(data$restecg,levels=c(0,1,2), labels = c("Normal", "ST 
                                                                Abnormality", 
                                                                "LVH"))
data$exang <- factor(data$exang,levels=c(0,1), labels = c("No", "Yes"))
data$slope <- factor(data$slope,levels=c(1,2,3), labels = c("Up-sloping", 
                                                            "Flat", 
                                                            "Down-sloping"))
data$cp <- factor(data$cp,levels=c(1,2,3,4), labels = c( "Typical angina",
                                                         "A-typical angina", 
                                                         "Non-Anginal pain", 
                                                         "Asymptomatic"))
data$class <- as.factor(data$class)
levels(data$class)[which(levels(data$class)=="0")] <- "No Disease"
levels(data$class)[which(levels(data$class)%in%c("1","2","3","4"))] <- "Disease"
```

Regression analysis is used to describe the relationship between a single 
dependent variable _Y_ (also called the outcome) and one or more independent 
variables _X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>p</sub>_ (also called the 
predictors).
The case of one independent variable, i.e. _p_=1, is known as simple regression 
and the case where _p_>1 is known as multiple regression.

## Simple Linear Regression

In simple linear regression, the aim is to predict the dependent variable _Y_ 
based on a given value of the independent variable _X_ using a linear 
relationship. This relationship is described by the following equation:

_Y = a + bX + e_

where _a_ is the intercept (the value of _Y_ when _X=0_) and _b_ is the 
regression coefficient (the slope of the line).
_e_ represents an additive residual term (error) that accounts for random 
statistical noise or the effect of unobserved independent variables not included 
in the model.

In simple linear regression, we wish to find the values of _a_ and _b_ that best 
describe the relationship given by the data.

This objective can be thought of as drawing a straight line that best fits the 
plotted data points.
The line can then be used to predict Y from X.

![RStudio layout](../fig/03-fig1.png)

## Least Squares Estimation

Ordinary least squares (OLS) is a common method used to determine values of _a_
and _b_ through minimising the sum of the squares of the residuals, which can be 
described mathematically as the problem $$ \min_{a,b} \sum_i (Y_i - (a + bX_i))^2 $$

Solving through calculus, the solution to this problem is $$ \hat{b} = \frac{\sum_i (x_i - \bar x) (y_i - \bar y)}{\sum_i (x_i - \bar x)^2} $$ and $$ \hat{a} = \bar y - \hat{b} \bar x $$ .

In the figure below, we have four observations and two lines attempting to fit 
the data.

![RStudio layout](../fig/03-fig2.png)

The sum of squares for the red sloped line is (2 - 1)<sup>2</sup> + (4 - 2)<sup>2</sup> + (1.5 - 3)<sup>2</sup> + (3.2 - 4)<sup>2</sup> = 7.9

and the sum of squares for the green flat line is (2 - 2.5)<sup>2</sup> + (4 - 2.5)<sup>2</sup> + (1.5 - 2.5)<sup>2</sup> + (3 - 2.5)<sup>2</sup> = 3.75

As the sum of squares for the green line is smaller, we can say it is a better 
fit to the data.

The coefficient _b_ is related to the correlation coefficient _r_: $$  r = b \frac{\text{SD}_X}{\text{SD}_Y} $$. If _X_ and _Y_ are positively correlated, then the slope _b_ is positive.

## Regression Hypothesis Test

We can estimate the most suitable values for our model, but how can we be 
confident that we have indeed found a significant linear relationship?
We can perform a hypothesis test that asks: Is the slope significantly different 
from 0?

**Hypothesis**

* H<sub>0</sub>: b = 0 (no linear relationship)
* H<sub>1</sub>: b â‰  0 (linear relationship)

**Test of Significance**: $$T = \hat{b}\ / \text{SE}(\hat{b})$$

**Result**: p-value significant or not

**Conclusion**: If p-value is significant, then the slope is different from 0 
and there is a significant linear relationship.

## Standard Error

The standard error of Y given X is the average variability around the regression 
line at any given value of X. It is assumed to be equal at all values of X.
Each observed residual can be thought of as an estimate of the actual unknown 
"true error" term.

![RStudio layout](../fig/03-fig3.png)


## Assumptions of Linear Regression

There are four assumptions associated with using linear models:

* the relationship between the independent and dependent variables is linear;
* the observations are independent;
* the variance of the residuals is constant across the observations (homoscedasticity);
* the residuals are normally distributed with mean 0.

These assumptions can be considered criteria that should be met before drawing inferences from the fitted model or using it to make predictions.

The residual e<sub>i</sub> of the i<sup>th</sup> observation is the difference between the observed dependent variable value Y<sub>i</sub> and the predicted value for the value of the independent variable(s) X<sub>i</sub>,

$$ e_i = Y_i - \hat{Y}_i = Y_i - (a + bX_i) $$

## Coefficient of Determination

The coefficient of determination _R<sup>2</sup>_ represents the proportion of the total sample variability (sum of squares) explained by the regression model.
This can also be interpreted as the proportion of the variance of the dependent variable that is explained by the independent variable(s).
The most general definition of the coefficient of determination is

$$ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} $$

where _SS<sub>res</sub> is the sum of the squares of the residuals as defined above and _SS<sub>tot</sub>_ is the total sum of squares $$ SS_{tot} = \sum_i (Y_i - \bar Y)^2 $$ which is proportional to the variance of the dependent variable.

## Example - Cigarettes and Coronary Heart Disease

*Example from Landwehr & Watkins (1987) and cited in Howell (2004)*

Let us consider the following research question: How fast does CHD mortality rise depending on smoking?

This can be framed as a linear regression problem, where each observation is a particular country and the independent variable is the average number of cigarettes smoked per adult per day and the dependent variable is the CHD mortality rate (deaths per 10,000 per year due to CHD).
The data we will use in this example is given in the table and visualised in the scatter plot, with a line of best fit, below:

Cig. | CHD
---- | ---
11   | 26
9    | 21
9    | 24
9    | 21
8    | 19
8    | 13
8    | 19
6    | 11
6    | 23
5    | 15
5    | 13
5    | 4
5    | 18
5    | 12
5    | 3
4    | 11
4    | 15
4    | 6
3    | 13
3    | 4
3    | 14

![RStudio layout](../fig/03-fig4.png)

As a reminder, the linear regression equation is $$ \hat Y = a + bX  $$

In this example, _Y_ is the CHD mortality rate (CHD) and _X_ is the average number of cigarettes smoked per adult per day (CigCons), so the equation is $$ \text{CHD}_{\text{pred}} = a + b \times \text{CigsCons} + e $$

The intercept _a_ represents the predicted CHD mortality rate in a nation where the average number of cigarettes smoked per adult per day is zero.
The slope _b_ represents the rate of increase of CHD mortality rate for each unit increase in the average number of cigarettes smoked per adult per day.

### Regression Results

The results of fitting a linear model to the data is shown below.

![RStudio layout](../fig/03-fig5.png)

The p-value associated with the prediction of the slope is less than 0.05, indicating there is significant evidence that the slope is different from zero, and therefore there is a significant relationship between cigarette consumption and CHD mortality.

Substituting these values into our linear regression equation, we get the predicted model

$$ \text{CHD}_{\text{pred}} = 2.37 + 2.04 \times \text{CigsCons} $$

We can interpret this as:

* CHD mortality is equal to 2.37 on average when 0 cigarettes are consumed;
* CHD mortality increases by an average of 2.04 units per unit increase in cigarette consumption.

### Making a Prediction

We may want to predict CHD mortality when cigarette consumption is 6, which can be done using our model.

$$ \text{CHD}_{\text{pred}} = 2.37 + 2.04 \times 6 = 14.61 $$

and so we predict that on average 14.61/10,000 people will die of CHD _per annum_ in a country with an average cigarette consumption of 6 per person per day.

### Residuals

Now that we have a fitted model, we can check the residuals between our model and the data we used to fit the model.
In simple regression, the residuals is the vertical distance between the actual and predicted values for each observation; an example of the residual for a particular observation is shown in the figure below.

![RStudio layout](../fig/03-fig6.png)

For one observation, there was a CHD mortality of 23 deaths per 10,000 with the average number of cigarettes smoked per adult per day is 6.
Our prediction for the same cigarette consumption level was 14.61 deaths per 10,000, and so the residual for this observation is 23 - 14.61 = 8.39.

For this model, we obtained an _R_ value of 0.71.
Squaring this, we get _R<sup>2</sup>_ = 0.51, and so we find that almost 50% of the variability of incidence of CHD mortality is associated with variability in smoking rates.

## Linear Regression R Functions

* `result <- lm(Y~X,data=data)`: Get estimated parameters of regression 
* `summary(result)`: Display results of linear regression
* `confint(result)`: Get confidence intervals around estimated coefficients
* `fitted(result)`: Get predicted values for Y
* `resid(result`: Get the residuals of the model

## Exercise - SBP and Age

In this exercise we will be exploring the relationship between systolic blood pressure (the dependent variable) and age (the independent variable) using linear models.

First, we will use a scatter plot to visually check that there is a linear relationship.

```{r, eval=FALSE}
plot(data$trestbps~data$age)
```
![RStudio layout](../fig/03-fig7.png)

The `lm` function is used to fit a linear model and obtain the estimated values of $a$ and $b$, and we can use the `summary` function to view a summary of the results.

```{r}
model1 <- lm(trestbps~age,data=data)

summary(model1)
```

Under the Coefficients heading, we can view information about the predicted variables and their significance.
We see that age has a significant effect on SBP ($p = 0.00139$) and for each year increase in age there is a 0.60 unit increase in SBP level.

The `confint` function gives us a 95% confidence interval for each model parameter estimate.

```{r}
confint(model1)
```

We can interpret the 95% confidence interval for the relationship with age as: We can be 95% confident that the range 0.24 to 0.97 contains the true value of the linear relationship between age and SBP.

The `coefficients` function gives us the model coefficients, as outputted by `summary`.

```{r}
coefficients(model1)
```

`fitted` gives us the predicted SBP values for each value of age in our data set.

```{r}
fitted(model1)
```

`resid` gives the residuals for each observation in our data set.

```{r}
resid(model1)
```

By using `abline` with our model after plotting a scatter plot, we can plot the estimated regression line over our data.

```{r, eval=FALSE}
plot(trestbps~age,data=data)
abline(model1,col="red")
```
![RStudio layout](../fig/03-fig8.png)

We can do the same with ggplot2.

```{r, eval=FALSE}
library(ggplot2)
ggplot(data, aes(x=age, y=trestbps)) +
  geom_point(shape=16,size=2) +
  geom_smooth(method=lm,se=F)+
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"))
```
![RStudio layout](../fig/03-fig9.png)

### Confidence and Prediction

We can use the `predict` function to predict SBP for each value of age in our data set and also get confidence or prediction intervals for each observation.
The type of interval is specified using the `int` argument to the function, with `int="c"` for confidence intervals and `int="p"` for prediction intervals.

Confidence intervals tell us how well we have determined the estimate parameters.
The standard error for a CI takes into account the uncertainty due to sampling.

Prediction intervals tell us in what range a future individual observation will fall.
The standard error for a PI takes into account the uncertainty due to sampling as well as the variability of the individuals around the predicted mean.

Both intervals will be centered around the same value but the standard errors will be different.
A prediction interval is always wider than a confidence interval. 

For example, here we obtain the confidence intervals for each prediction.

```{r, output.lines=11}
predict(model1, int="c")
```

Note that when we have a model fitted using `lm`, the `predict` and `fitted` functions give the same predicted values.

## What can go wrong?

As discussed, there are some assumptions that need to be met to use a linear model and we must check if they are being violated before using our model to draw inferences or make predictions.
Such violations are:

* In the linear regression model:
  - Non-linearity (e.g. a quadratic relation or higher order terms).
* In the residual assumptions:
  - Non-normal distribution;
  - Non-constant variances;
  - Dependency;
  - Outliers.
  
The checks we need to make are:

* Linearity between outcome and predictors
  - Plot residuals vs. predictor variable (if non-linearity try higher-order terms in that variable)
* Constant variance of residuals
  - Plot residuals vs. fitted values (if variance increases with the response, apply a transformation to the data)
* Normality of residuals
  - Plot residuals Q-Q norm plot (significant deviation from a straight line indicates non-normality)
  
### Making the Checks

We can make the above checks manually by creating the necessary plots.

**Linearity between outcome and predictor**: Residuals vs predictor variable

```{r, eval = FALSE}
plot(resid(model1)~age,data=data)
```
![RStudio layout](../fig/03-fig10.png)

**Constant variance of residuals**: Residuals vs fitted values

```{r, eval=FALSE}
plot(resid(model1)~fitted(model1))
```
![RStudio layout](../fig/03-fig11.png)

**Normality of residuals**: Q-Q plot of residuals

```{r, eval=FALSE}
qqnorm(resid(model1))
```
![RStudio layout](../fig/03-fig12.png)

Or, we can use R's built-in diagnostic plotting with the `plot` function.

```{r, eval = FALSE}
par(mfrow=c(2,2))
plot(model1)
```
![RStudio layout](../fig/03-fig13.png)

Let's break down each of these plots.

For the *Residuals vs Fitted* plot, the residuals should be randomly distributed around the horizontal line representing a residual error of zero; that is, there should not be a distinct trend in the distribution of points.
Good and bad examples of these plots are shown in the figures below.

![RStudio layout](../fig/03-fig14.png)

![RStudio layout](../fig/03-fig15.png)

![RStudio layout](../fig/03-fig16.png)

*Normal Q-Q* plots the ordered standardised residuals against their theoretical expectations.
We expect these points to lie on a straight line, with significant deviation from a straight line indicating non-normality.

*Scale-Location* plots the square root of the standardised residuals (think of the square root of a measure of relative error) as a function of the fitted values.
Again, there should be no obvious trend in this plot.

*Point Leverage* (Residuals vs Leverage) gives us a measure of importance of each point in determining the regression result. 
If any point in this plot falls outside of a Cookâ€™s distance of 1 (indicated by the red dashed lines) then it is conventionally considered to be an influential observation.

## Leverage Points 

* Leverage points are observations which have an _x_-value that is distant from other _x_-values.
* These points have great influence on the fitted model, in that if they were to be removed from the data set and the model refitted, the resulting model would be significantly different from the original.
* Leverage points are bad if it is also an outlier (the _y_-value does not follow the pattern set by the other data points).
* Never remove data points unless we are certain that the data point is invalid and could not occur in real life.
* Otherwise, we should fit a different regression model if we are not confident that the basic linear model is suitable.
* We can create a different regression model by including higher-order terms or transforming the data.

## Cook's Distance

The Cook's distance statistic _D_ combines the effects of leverage and the magnitude of the residual and is used to evaluate the impact of a given observation on the estimated regression coefficient.
As a rule of thumb, a point with Cook's distance greater than 1 has undue influence on the model.

## Statistical Tests for Model Assumptions

There are a number of statistical tests available from the `lmtest` package that can be used to test for violations of the assumptions of a linear model.

For the Breusch-Pagan test of homoscedesiticity, the null hypothesis is the variance is unchanging in the residual.

```{r, message=FALSE}
library(lmtest)
bptest(model1, ~ age, data = data, studentize = FALSE)
```

For the Breusch-Godfrey test for higher-order serial correlation, the null hypothesis is the residuals are independent.

```{r, message=FALSE}
library(lmtest)
bgtest(model1, order = 1, order.by = NULL, type = c("Chisq", "F"), data = list())
```

For the Goldfeld-Quandt test against heteroscedasticity, the null hypothesis is the errors are homoscedastic (equal variance).

```{r, message=FALSE}
library(lmtest)
gqtest(model1, point = 0.5, fraction = 0, alternative = c("greater", "two.sided", "less"),
       order.by = NULL, data = list())
```

Each of these tests show that the models assumptions are met.

There are other packages you can use to test the assumptions of the model, such as the `car` library.
``qqPlot` gives us a QQ plot with 95% confidence intervals.

```{r, message=FALSE, eval=FALSE}
library(car)
qqPlot(model1, main="QQ Plot")
```
![RStudio layout](../fig/03-fig17.png)

`outlierTest` tests for outliers and prints a p-value for the most extreme outlier.

```{r}
outlierTest(model1)
```

In this case, we observe no significant outliers.

## Exercise - SBP and Sex

In this exercise we will be exploring the relationship between systolic blood pressure (the dependent variable) and sex (the independent variable) using linear models.
The main difference between this exercise and the last is that age is a numerical variable and sex is binary.

Comparing boxplots can give us a good indication if the distributions of SBP is different between males and females.

```{r, eval=FALSE}
ggplot(data = data, aes(x = sex,y = trestbps, fill="blues")) + 
  geom_boxplot()
```
![RStudio layout](../fig/03-fig18.png)

Visually, it seems that females on average have a higher systolic blood pressure than males.
The male distribution seems to be right-skewed.
We can test for normality for both variables using `shapiro.test`.

```{r}
by(data$trestbps,data$sex, shapiro.test)
```

We see here that there is significant evidence that the male dataset is non-normally distributed, and so we cannot use a t-test to determine if the means are different.
The Wilcoxon rank sum test, given by `wilcox.test` in R, is a non-parametric alternative to the unpaired two-samples t-test.

```{r}
wilcox.test(data$trestbps~data$sex)
```

The p-value of approximately 0.08 indicates that male's and female's systolic blood pressure are not significantly different.

Now, letâ€™s run a regression model to test the relationship between SBP and sex when SBP is considered as the outcome variable.

```{r}
model_sex <- lm(trestbps~sex,data=data)
summary(model_sex)
```

In this model, the first level of sex has been taken as reference (Females).
We can think of females being assigned a value of 0 in the linear model and males a value of 1.
Therefore, the model estimates that females, on average, have a systolic blood pressure of __136.76__, and for males __136.76 - 5.07 = 131.69__.
However, we see that the p-value for the sex effect is not significant, and so we can conclude that there is no difference in SBP between men and women.

Again, we can view and report the confidence intervals for the model coefficients for the model using `confint`.

```{r}
confint(model_sex)
```

And we can check the assumptions of the model by plotting the model diagnostics.

```{r, eval=FALSE}
par(mfrow=c(2,2))
plot(model_sex)
```
![RStudio layout](../fig/03-fig19.png)

Intepreting these model diagnostics is a bit harder, as we are using a binary predictor and thus only fitted values for the entire dataset.

* The variance of the residuals appears to be similar between the sexes.
* Our normal Q-Q plot shows little deviation from a straight line.
* In our Residuals vs. Leverage plot, no points fall outside of Cook's distance.
